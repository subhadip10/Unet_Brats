{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b9a8f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-18 14:04:47.488818: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import nibabel as nib\n",
    "from tqdm import notebook,tqdm\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from keras_unet_collection import models,utils,losses\n",
    "import keras\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "np.set_printoptions(precision=3, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ea1a13",
   "metadata": {},
   "source": [
    "## Using custom Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2070c842",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATASET_PATH = \"/Users/s0p00zp/Documents/Mtech/Capstone/archive/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22a67b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOLUME_SLICES = 100 \n",
    "VOLUME_START_AT = 22 # first slice of volume that we will include\n",
    "\n",
    "IMG_SIZE=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "193198df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lists of directories with studies\n",
    "train_and_val_directories = [f.path for f in os.scandir(TRAIN_DATASET_PATH) if f.is_dir()]\n",
    "\n",
    "# file BraTS20_Training_355 has ill formatted name for for seg.nii file\n",
    "#train_and_val_directories.remove(TRAIN_DATASET_PATH+'BraTS20_Training_355')\n",
    "\n",
    "\n",
    "def pathListIntoIds(dirList):\n",
    "    x = []\n",
    "    for i in range(0,len(dirList)):\n",
    "        x.append(dirList[i][dirList[i].rfind('/')+1:])\n",
    "    return x\n",
    "\n",
    "train_and_test_ids = pathListIntoIds(train_and_val_directories); \n",
    "\n",
    "    \n",
    "train_test_ids, val_ids = train_test_split(train_and_test_ids,test_size=0.2) \n",
    "train_ids, test_ids = train_test_split(train_test_ids,test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5f35605",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, dim=(IMG_SIZE,IMG_SIZE), batch_size = 1, n_channels = 1, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_channels = n_channels\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        Batch_ids = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(Batch_ids)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, Batch_ids):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.zeros((self.batch_size*VOLUME_SLICES, *self.dim, self.n_channels))\n",
    "        y = np.zeros((self.batch_size*VOLUME_SLICES, 240, 240))\n",
    "        Y = np.zeros((self.batch_size*VOLUME_SLICES, *self.dim, 4))\n",
    "\n",
    "        \n",
    "        # Generate data\n",
    "        for c, i in enumerate(Batch_ids):\n",
    "            case_path = os.path.join(TRAIN_DATASET_PATH, i)\n",
    "\n",
    "            data_path = os.path.join(case_path, f'{i}_flair.nii')\n",
    "            flair = nib.load(data_path).get_fdata()    \n",
    "\n",
    "#             data_path = os.path.join(case_path, f'{i}_t1ce.nii')\n",
    "#             ce = nib.load(data_path).get_fdata()\n",
    "            \n",
    "#             data_path = os.path.join(case_path, f'{i}_t1.nii')\n",
    "#             t1 = nib.load(data_path).get_fdata()\n",
    "            \n",
    "            data_path = os.path.join(case_path, f'{i}_seg.nii')\n",
    "            seg = nib.load(data_path).get_fdata()\n",
    "        \n",
    "            for j in range(VOLUME_SLICES):\n",
    "                X[j +VOLUME_SLICES*c,:,:,0] = cv2.resize(flair[:,:,j+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE))\n",
    "                #X[j +VOLUME_SLICES*c,:,:,1] = cv2.resize(ce[:,:,j+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE));\n",
    "                #X[j +VOLUME_SLICES*c,:,:,2] = cv2.resize(t1[:,:,j+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE));\n",
    "\n",
    "                y[j +VOLUME_SLICES*c] = seg[:,:,j+VOLUME_START_AT]\n",
    "                    \n",
    "        # Generate masks\n",
    "        y[y==4] = 3\n",
    "        mask = tf.one_hot(y, 4)\n",
    "        Y = tf.image.resize(mask, (IMG_SIZE, IMG_SIZE))\n",
    "        return X/np.max(X), Y\n",
    "        \n",
    "training_generator = DataGenerator(train_ids)\n",
    "valid_generator = DataGenerator(val_ids)\n",
    "test_generator = DataGenerator(test_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f65c8522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dice loss as defined above for 4 classes\n",
    "def dice_coef(y_true, y_pred, smooth=1.0):\n",
    "    class_num = 4\n",
    "    for i in range(class_num):\n",
    "        y_true_f = K.flatten(y_true[:,:,:,i])\n",
    "        y_pred_f = K.flatten(y_pred[:,:,:,i])\n",
    "        intersection = K.sum(y_true_f * y_pred_f)\n",
    "        loss = ((2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth))\n",
    "   #     K.print_tensor(loss, message='loss value for class {} : '.format(SEGMENT_CLASSES[i]))\n",
    "        if i == 0:\n",
    "            total_loss = loss\n",
    "        else:\n",
    "            total_loss = total_loss + loss\n",
    "            \n",
    "    total_loss = total_loss / class_num\n",
    "#    K.print_tensor(total_loss, message=' total dice coef: ')\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42511e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_unet(inputs, ker_init, dropout):\n",
    "    conv1 = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(inputs)\n",
    "    conv1 = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(conv1)\n",
    "    \n",
    "    pool = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    conv = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(pool)\n",
    "    conv = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(conv)\n",
    "    \n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv)\n",
    "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(pool1)\n",
    "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(conv2)\n",
    "    \n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(pool2)\n",
    "    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(conv3)\n",
    "    \n",
    "    \n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    conv5 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(pool4)\n",
    "    conv5 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(conv5)\n",
    "    drop5 = Dropout(dropout)(conv5)\n",
    "\n",
    "    up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(UpSampling2D(size = (2,2))(drop5))\n",
    "    merge7 = concatenate([conv3,up7], axis = 3)\n",
    "    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(merge7)\n",
    "    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(conv7)\n",
    "\n",
    "    up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(UpSampling2D(size = (2,2))(conv7))\n",
    "    merge8 = concatenate([conv2,up8], axis = 3)\n",
    "    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(merge8)\n",
    "    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(conv8)\n",
    "\n",
    "    up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(UpSampling2D(size = (2,2))(conv8))\n",
    "    merge9 = concatenate([conv,up9], axis = 3)\n",
    "    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(merge9)\n",
    "    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(conv9)\n",
    "    \n",
    "    up = Conv2D(32, 2, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(UpSampling2D(size = (2,2))(conv9))\n",
    "    merge = concatenate([conv1,up], axis = 3)\n",
    "    conv = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(merge)\n",
    "    conv = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(conv)\n",
    "    \n",
    "    conv10 = Conv2D(4, (1,1), activation = 'softmax')(conv)\n",
    "    \n",
    "    return Model(inputs = inputs, outputs = conv10)\n",
    "\n",
    "input_layer = Input((IMG_SIZE, IMG_SIZE, 1))\n",
    "\n",
    "model = build_unet(input_layer, 'he_normal', 0.2)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), \n",
    "              metrics = ['accuracy',tf.keras.metrics.MeanIoU(num_classes=4), dice_coef] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6fb0dd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ef6091a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-18 14:10:20.443930: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - ETA: 0s - loss: 0.0773 - accuracy: 0.9827 - mean_io_u: 0.3756 - dice_coef: 0.2895 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-18 15:00:39.424199: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 3254s 13s/step - loss: 0.0773 - accuracy: 0.9827 - mean_io_u: 0.3756 - dice_coef: 0.2895 - val_loss: 0.0625 - val_accuracy: 0.9827 - val_mean_io_u: 0.3756 - val_dice_coef: 0.3074\n",
      "Epoch 2/30\n",
      "250/250 [==============================] - 3314s 13s/step - loss: 0.0552 - accuracy: 0.9838 - mean_io_u: 0.3756 - dice_coef: 0.3355 - val_loss: 0.0449 - val_accuracy: 0.9865 - val_mean_io_u: 0.3756 - val_dice_coef: 0.3467\n",
      "Epoch 3/30\n",
      "250/250 [==============================] - 3290s 13s/step - loss: 0.0481 - accuracy: 0.9858 - mean_io_u: 0.3756 - dice_coef: 0.3557 - val_loss: 0.0392 - val_accuracy: 0.9871 - val_mean_io_u: 0.3756 - val_dice_coef: 0.3854\n",
      "Epoch 4/30\n",
      "250/250 [==============================] - 3154s 13s/step - loss: 0.0461 - accuracy: 0.9859 - mean_io_u: 0.3756 - dice_coef: 0.3662 - val_loss: 0.0470 - val_accuracy: 0.9851 - val_mean_io_u: 0.3756 - val_dice_coef: 0.3765\n",
      "Epoch 5/30\n",
      "250/250 [==============================] - 3221s 13s/step - loss: 0.0412 - accuracy: 0.9869 - mean_io_u: 0.3756 - dice_coef: 0.3819 - val_loss: 0.0349 - val_accuracy: 0.9887 - val_mean_io_u: 0.3756 - val_dice_coef: 0.3945\n",
      "Epoch 6/30\n",
      "250/250 [==============================] - 3182s 13s/step - loss: 0.0385 - accuracy: 0.9875 - mean_io_u: 0.3756 - dice_coef: 0.3941 - val_loss: 0.0357 - val_accuracy: 0.9889 - val_mean_io_u: 0.3756 - val_dice_coef: 0.4039\n",
      "Epoch 7/30\n",
      "250/250 [==============================] - 3173s 13s/step - loss: 0.0384 - accuracy: 0.9876 - mean_io_u: 0.3756 - dice_coef: 0.3949 - val_loss: 0.0407 - val_accuracy: 0.9875 - val_mean_io_u: 0.3756 - val_dice_coef: 0.3875\n",
      "Epoch 8/30\n",
      "250/250 [==============================] - 3147s 13s/step - loss: 0.0343 - accuracy: 0.9881 - mean_io_u: 0.3756 - dice_coef: 0.4104 - val_loss: 0.0319 - val_accuracy: 0.9891 - val_mean_io_u: 0.3756 - val_dice_coef: 0.4149\n",
      "Epoch 9/30\n",
      "250/250 [==============================] - 7275s 29s/step - loss: 0.0328 - accuracy: 0.9884 - mean_io_u: 0.3756 - dice_coef: 0.4183 - val_loss: 0.0304 - val_accuracy: 0.9893 - val_mean_io_u: 0.3756 - val_dice_coef: 0.4290\n",
      "Epoch 10/30\n",
      "250/250 [==============================] - 3961s 16s/step - loss: 0.0400 - accuracy: 0.9870 - mean_io_u: 0.3756 - dice_coef: 0.3970 - val_loss: 0.0347 - val_accuracy: 0.9879 - val_mean_io_u: 0.3756 - val_dice_coef: 0.3966\n",
      "Epoch 11/30\n",
      "250/250 [==============================] - 3258s 13s/step - loss: 0.0320 - accuracy: 0.9884 - mean_io_u: 0.3756 - dice_coef: 0.4213 - val_loss: 0.0292 - val_accuracy: 0.9897 - val_mean_io_u: 0.3756 - val_dice_coef: 0.4323\n",
      "Epoch 12/30\n",
      "250/250 [==============================] - 3213s 13s/step - loss: 0.0303 - accuracy: 0.9887 - mean_io_u: 0.3756 - dice_coef: 0.4316 - val_loss: 0.0300 - val_accuracy: 0.9893 - val_mean_io_u: 0.3756 - val_dice_coef: 0.4220\n",
      "Epoch 13/30\n",
      "250/250 [==============================] - 3147s 13s/step - loss: 0.0298 - accuracy: 0.9889 - mean_io_u: 0.3756 - dice_coef: 0.4333 - val_loss: 0.0303 - val_accuracy: 0.9895 - val_mean_io_u: 0.3756 - val_dice_coef: 0.4210\n",
      "Epoch 14/30\n",
      "250/250 [==============================] - 3456s 14s/step - loss: 0.0326 - accuracy: 0.9887 - mean_io_u: 0.3756 - dice_coef: 0.4249 - val_loss: 0.0322 - val_accuracy: 0.9891 - val_mean_io_u: 0.3756 - val_dice_coef: 0.4180\n",
      "Epoch 15/30\n",
      "250/250 [==============================] - 3479s 14s/step - loss: 0.0365 - accuracy: 0.9878 - mean_io_u: 0.3756 - dice_coef: 0.4116 - val_loss: 0.0306 - val_accuracy: 0.9892 - val_mean_io_u: 0.3756 - val_dice_coef: 0.4263\n",
      "Epoch 16/30\n",
      "250/250 [==============================] - 3530s 14s/step - loss: 0.0303 - accuracy: 0.9890 - mean_io_u: 0.3756 - dice_coef: 0.4323 - val_loss: 0.0278 - val_accuracy: 0.9903 - val_mean_io_u: 0.3756 - val_dice_coef: 0.4424\n",
      "Epoch 17/30\n",
      "250/250 [==============================] - 3489s 14s/step - loss: 0.0294 - accuracy: 0.9893 - mean_io_u: 0.3756 - dice_coef: 0.4387 - val_loss: 0.0286 - val_accuracy: 0.9900 - val_mean_io_u: 0.3756 - val_dice_coef: 0.4435\n",
      "Epoch 18/30\n",
      "250/250 [==============================] - 3414s 14s/step - loss: 0.0275 - accuracy: 0.9898 - mean_io_u: 0.3756 - dice_coef: 0.4462 - val_loss: 0.0285 - val_accuracy: 0.9900 - val_mean_io_u: 0.3756 - val_dice_coef: 0.4485\n",
      "Epoch 19/30\n",
      "250/250 [==============================] - 12668s 51s/step - loss: 0.0263 - accuracy: 0.9902 - mean_io_u: 0.3756 - dice_coef: 0.4562 - val_loss: 0.0266 - val_accuracy: 0.9905 - val_mean_io_u: 0.3756 - val_dice_coef: 0.4509\n",
      "Epoch 20/30\n",
      "250/250 [==============================] - 2800s 11s/step - loss: 0.0256 - accuracy: 0.9902 - mean_io_u: 0.3756 - dice_coef: 0.4606 - val_loss: 0.0265 - val_accuracy: 0.9906 - val_mean_io_u: 0.3756 - val_dice_coef: 0.4492\n",
      "Epoch 21/30\n",
      "250/250 [==============================] - 2758s 11s/step - loss: 0.0260 - accuracy: 0.9903 - mean_io_u: 0.3756 - dice_coef: 0.4598 - val_loss: 0.0307 - val_accuracy: 0.9898 - val_mean_io_u: 0.3756 - val_dice_coef: 0.4560\n",
      "Epoch 22/30\n",
      "250/250 [==============================] - 2796s 11s/step - loss: 0.0260 - accuracy: 0.9902 - mean_io_u: 0.3756 - dice_coef: 0.4588 - val_loss: 0.0266 - val_accuracy: 0.9907 - val_mean_io_u: 0.3756 - val_dice_coef: 0.4620\n",
      "Epoch 23/30\n",
      "250/250 [==============================] - 2974s 12s/step - loss: 0.0255 - accuracy: 0.9904 - mean_io_u: 0.3756 - dice_coef: 0.4638 - val_loss: 0.0263 - val_accuracy: 0.9907 - val_mean_io_u: 0.3756 - val_dice_coef: 0.4576\n",
      "Epoch 24/30\n",
      "250/250 [==============================] - 12009s 48s/step - loss: 0.0247 - accuracy: 0.9906 - mean_io_u: 0.3756 - dice_coef: 0.4702 - val_loss: 0.0271 - val_accuracy: 0.9905 - val_mean_io_u: 0.3756 - val_dice_coef: 0.4626\n",
      "Epoch 25/30\n",
      "250/250 [==============================] - 3361s 13s/step - loss: 0.0332 - accuracy: 0.9889 - mean_io_u: 0.3756 - dice_coef: 0.4332 - val_loss: 0.0325 - val_accuracy: 0.9889 - val_mean_io_u: 0.3756 - val_dice_coef: 0.4223\n",
      "Epoch 26/30\n",
      "250/250 [==============================] - 3627s 15s/step - loss: 0.0287 - accuracy: 0.9897 - mean_io_u: 0.3756 - dice_coef: 0.4486 - val_loss: 0.0260 - val_accuracy: 0.9905 - val_mean_io_u: 0.3756 - val_dice_coef: 0.4644\n",
      "Epoch 27/30\n",
      "250/250 [==============================] - 3579s 14s/step - loss: 0.0244 - accuracy: 0.9906 - mean_io_u: 0.3756 - dice_coef: 0.4717 - val_loss: 0.0267 - val_accuracy: 0.9908 - val_mean_io_u: 0.3756 - val_dice_coef: 0.4673\n",
      "Epoch 28/30\n",
      "250/250 [==============================] - 3135s 13s/step - loss: 0.0241 - accuracy: 0.9907 - mean_io_u: 0.3756 - dice_coef: 0.4748 - val_loss: 0.0292 - val_accuracy: 0.9893 - val_mean_io_u: 0.3756 - val_dice_coef: 0.4556\n",
      "Epoch 29/30\n",
      "250/250 [==============================] - 6046s 24s/step - loss: 0.0238 - accuracy: 0.9908 - mean_io_u: 0.3756 - dice_coef: 0.4803 - val_loss: 0.0267 - val_accuracy: 0.9903 - val_mean_io_u: 0.3756 - val_dice_coef: 0.4665\n",
      "Epoch 30/30\n",
      "250/250 [==============================] - 3277s 13s/step - loss: 0.0230 - accuracy: 0.9910 - mean_io_u: 0.3756 - dice_coef: 0.4853 - val_loss: 0.0254 - val_accuracy: 0.9910 - val_mean_io_u: 0.3756 - val_dice_coef: 0.4764\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "filepath=\"2D-UNet-{epoch:02d}-{val_accuracy:.3f}.hdf5\" \n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, verbose=1, restore_best_weights=True)\n",
    "\n",
    "#csv_logger = CSVLogger('training_2021_2D_UNet.log')\n",
    "\n",
    "history =  model.fit(training_generator,\n",
    "                    epochs=30,\n",
    "                    steps_per_epoch=len(train_ids),\n",
    "                    validation_data = valid_generator\n",
    "                    )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d2e0308",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model_2023_2D_UNet.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1894f95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "layout_parser",
   "language": "python",
   "name": "layout_parser"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
